"""load_steinmetz_decisions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/master/projects/load_steinmetz_decisions.ipynb

## Loading of Steinmetz data

includes some visualizations
"""

#%% Retrieves main data set
import os, requests
import numpy as np
import pandas as pd
from matplotlib import rcParams 
from matplotlib import pyplot as plt
import seaborn as sns
import elephant
%matplotlib

fname = []
for j in range(3):
  fname.append('steinmetz_part%d.npz'%j)
url = ["https://osf.io/agvxh/download"]
url.append("https://osf.io/uv3mw/download")
url.append("https://osf.io/ehmw2/download")

for j in range(len(url)):
  if not os.path.isfile(fname[j]):
    try:
      r = requests.get(url[j])
    except requests.ConnectionError:
      print("!!! Failed to download data !!!")
    else:
      if r.status_code != requests.codes.ok:
        print("!!! Failed to download data !!!")
      else:
        with open(fname[j], "wb") as fid:
          fid.write(r.content)

fname = ['steinmetz_st.npz']
fname.append('steinmetz_wav.npz')
fname.append('steinmetz_lfp.npz')

url = ["https://osf.io/4bjns/download"]
url.append("https://osf.io/ugm9v/download")
url.append("https://osf.io/kx3v9/download")

for j in range(len(url)):
  if not os.path.isfile(fname[j]):
    try:
      r = requests.get(url[j])
    except requests.ConnectionError:
      print("!!! Failed to download data !!!")
    else:
      if r.status_code != requests.codes.ok:
        print("!!! Failed to download data !!!")
      else:
        with open(fname[j], "wb") as fid:
          fid.write(r.content)

all_dat_LFP = np.load('steinmetz_lfp.npz', allow_pickle=True)['dat']
all_dat_WAV = np.load('steinmetz_wav.npz', allow_pickle=True)['dat']
all_dat_ST = np.load('steinmetz_st.npz', allow_pickle=True)['dat']

#@title Data loading

alldat = np.array([])
for j in range(len(fname)):
  alldat = np.hstack((alldat, np.load('steinmetz_part%d.npz'%j, allow_pickle=True)['dat']))

# select just one of the recordings here. 11 is nice because it has some neurons in vis ctx. 


"""`alldat` contains 39 sessions from 10 mice, data from Steinmetz et al, 2019. Time bins for all measurements are 10ms, starting 500ms before stimulus onset. The mouse had to determine which side has the highest contrast. For each `dat = alldat[k]`, you have the fields below. For extra variables, check out the extra notebook and extra data files (lfp, waveforms and exact spike times, non-binned). 

* `dat['mouse_name']`: mouse name
* `dat['date_exp']`: when a session was performed
* `dat['spks']`: neurons by trials by time bins.    
* `dat['brain_area']`: brain area for each neuron recorded. 
* `dat['ccf']`: Allen Institute brain atlas coordinates for each neuron. 
* `dat['ccf_axes']`: axes names for the Allen CCF. 
* `dat['contrast_right']`: contrast level for the right stimulus, which is always contralateral to the recorded brain areas.
* `dat['contrast_left']`: contrast level for left stimulus. 
* `dat['gocue']`: when the go cue sound was played. 
* `dat['response_time']`: when the response was registered, which has to be after the go cue. The mouse can turn the wheel before the go cue (and nearly always does!), but the stimulus on the screen won't move before the go cue.  
* `dat['response']`: which side the response was (`-1`, `0`, `1`). When the right-side stimulus had higher contrast, the correct choice was `-1`. `0` is a no go response. 
* `dat['feedback_time']`: when feedback was provided. 
* `dat['feedback_type']`: if the feedback was positive (`+1`, reward) or negative (`-1`, white noise burst).  
* `dat['wheel']`: turning speed of the wheel that the mice uses to make a response, sampled at `10ms`. 
* `dat['pupil']`: pupil area  (noisy, because pupil is very small) + pupil horizontal and vertical position.
* `dat['face']`: average face motion energy from a video camera. 
* `dat['licks']`: lick detections, 0 or 1.   
* `dat['trough_to_peak']`: measures the width of the action potential waveform for each neuron. Widths `<=10` samples are "putative fast spiking neurons". 
* `dat['%X%_passive']`: same as above for `X` = {`spks`, `pupil`, `wheel`, `contrast_left`, `contrast_right`} but for  passive trials at the end of the recording when the mouse was no longer engaged and stopped making responses. 
* `dat['prev_reward']`: time of the feedback (reward/white noise) on the previous trial in relation to the current stimulus time. 
* `dat['reaction_time']`: ntrials by 2. First column: reaction time computed from the wheel movement as the first sample above `5` ticks/10ms bin. Second column: direction of the wheel movement (`0` = no move detected).

See https://github.com/nsteinme/steinmetz-et-al-2019/wiki/data-files for description of dataset

trial timeline:
1. mouse holds wheel still for a short interval (0.2-0.5s)
2. trial initiates with stimulus onset
3. after stimulus onset, there is random delay interval of 0.5-1.2s
4. at end of delay interval, auditory tone cue is delivered (Go cue)
5. in the following 1.5s, mouse moves wheel (or doesn't, if no stimulus presented) 
6. feedback is delivered at the end of 1.5s, for 1s duration
7. 1s inter-trial interval
8. mouse can initiate another trial by holding the wheel still

windows of interest for firing rates:
- baseline: -0.2 to 0s relative to stimulus onset
- trial firing rate: stimulus onset to 400 ms post-stimulus
- stimulus-driven: 0.05 to 0.15s after stimulus onset
- pre-movement FR: -0.1 to 0.05s relative to movement onset
- post-movement FR: -0.05 to 0.2s relative to movement onset
- post-reward rate: 0 to 0.15s relative to reward delivery for correct NoGos
"""

"""`dat_LFP`, `dat_WAV`, `dat_ST` contain 39 sessions from 10 mice, data from Steinmetz et al, 2019, supplemental to the main data provided for NMA. Time bins for all measurements are 10ms, starting 500ms before stimulus onset (same as the main data). The followin fields are available across the three supplemental files. 

* `dat['lfp']`: recording of the local field potential in each brain area from this experiment, binned at `10ms`.
* `dat['brain_area_lfp']`: brain area names for the LFP channels. 
* `dat['trough_to_peak']`: measures the width of the action potential waveform for each neuron. Widths `<=10` samples are "putative fast spiking neurons". 
* `dat['waveform_w']`: temporal components of spike waveforms. `w@u` reconstructs the time by channels action potential shape. 
* `dat['waveform_u]`: spatial components of spike waveforms.
* `dat['ss']`: neurons by trials. Exact spikes times for each neuron and each trial, reference to the stimulus onset. A (neuron,trial) entry can be an empty list if that neuron did not fire at all on that trial. 
* `dat['%X%_passive']`: same as above for `X` = {`lfp`, `ss`} but for  passive trials at the end of the recording when the mouse was no longer engaged and stopped making responses.
"""

# groupings of brain regions
regions = ["vis ctx", "thal", "hipp", "other ctx", "midbrain", "basal ganglia", "cortical subplate", "other"]
brain_groups = [["VISa", "VISam", "VISl", "VISp", "VISpm", "VISrl"], # visual cortex
                ["CL", "LD", "LGd", "LH", "LP", "MD", "MG", "PO", "POL", "PT", "RT", "SPF", "TH", "VAL", "VPL", "VPM"], # thalamus
                ["CA", "CA1", "CA2", "CA3", "DG", "SUB", "POST"], # hippocampal
                ["ACA", "AUD", "COA", "DP", "ILA", "MOp", "MOs", "OLF", "ORB", "ORBm", "PIR", "PL", "SSp", "SSs", "RSP"," TT"], # non-visual cortex
                ["APN", "IC", "MB", "MRN", "NB", "PAG", "RN", "SCs", "SCm", "SCig", "SCsg", "ZI"], # midbrain
                ["ACB", "CP", "GPe", "LS", "LSc", "LSr", "MS", "OT", "SNr", "SI"], # basal ganglia 
                ["BLA", "BMA", "EP", "EPd", "MEA"] # cortical subplate
                ]


session_number = 11
dat = alldat[session_number]
region = "VISam"

dat_LFP = all_dat_LFP[session_number]
dat_WAV = all_dat_WAV[session_number]
dat_ST = all_dat_ST[session_number]

all_trials_window_spikes = {} # creates dict of dicts
all_trials_window_counts = pd.DataFrame()

# gets spike times and counts for all neurons in all trials for region of interest

for trial_number in range(len(dat['response_time'])):

    # for all neurons, extract spike counts in the window [t-100ms, t], where t is the response time
    trial_response_time = dat['response_time'][trial_number] # response time for the first trial
    pre_move_window_start = trial_response_time - 0.1

    all_spike_times = dat_ST['ss'][dat['brain_area']==region][:,trial_number] # spike times for the first trial in all neurons

    all_spikes_in_window = []
    all_counts_in_window = []
    neuron_ids = np.where(dat['brain_area']==region)[0]
    all_trials_window_counts[trial_number] = len(dat['response_time'])

    for cell in range(len(all_spike_times)):
        mask = np.logical_and(all_spike_times[cell] >= pre_move_window_start, all_spike_times[cell] <= trial_response_time)
        cell_window_counts = np.sum(mask)
        cell_window_spikes = all_spike_times[cell][mask]
        all_counts_in_window.append(cell_window_counts)
        all_spikes_in_window.append(cell_window_spikes)
  
    one_trial_window_spikes = dict(zip(neuron_ids, all_spikes_in_window)) # each cells is a key in nested dict
    all_trials_window_counts[trial_number] = all_counts_in_window
    all_trials_window_counts.index = neuron_ids

    all_trials_window_spikes[trial_number] = one_trial_window_spikes # each trial is a dict in the dict
    #all_trials_window_counts[trial_number] = one_trial_window_counts

decision_time = 100

response_t = dat['response_time']
response_t_ms = response_t * 1000

bin_size_ms = dat['bin_size'] * 1000  # 10ms

# Bin in which the response occurs for all of the neurons in the region
response_t_bins = (response_t_ms / bin_size_ms)
response_t_bins = np.ceil(response_t_bins).astype(int)
number_of_bins_for_analysis = int(decision_time / bin_size_ms)


trial_summed_spike_counts = np.sum(all_trials_window_counts.T, axis=1) #total spike counts in 100 ms decision window
trial_summed_spike_counts = np.asarray(trial_summed_spike_counts)

avg_pupil_areas_in_decision = []

for trial_number in range(len(dat['response_time'])):
  pupil_areas_in_decision = dat['pupil'][0][trial_number][int(response_t_bins[trial_number])-number_of_bins_for_analysis:int(response_t_bins[trial_number])]
  avg_pupil_area = np.mean(pupil_areas_in_decision)
  avg_pupil_areas_in_decision.append(avg_pupil_area)

y_response = dat['response']

x_array = pd.DataFrame()
x_array['pupil_area'] = avg_pupil_areas_in_decision
x_array['total_spk_counts'] = trial_summed_spike_counts



def make_design_matrix(x):
  """Create the design matrix of inputs for use in polynomial regression
  
  Args:
    x (ndarray): input vector of shape (samples,) 
    order (scalar): polynomial regression order
  Returns:
    ndarray: design matrix for polynomial regression of shape (samples, order+1)
  """

  # Broadcast to shape (n x 1) so dimensions work 
  if x.ndim == 1:
    x = x[:, None]

  #if x has more than one feature, we don't want multiple columns of ones so we assign
  # x^0 here
  design_matrix = pd.DataFrame(np.ones((x.shape[0], 1))) 
  design_matrix = pd.concat([design_matrix, x], axis=1)

  return design_matrix

X = make_design_matrix(x_array)


from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression(penalty="none")
log_reg.fit(X, y_response)
y_pred = log_reg.predict(X)



def compute_accuracy(X, y, model):
  """Compute accuracy of classifier predictions.
  
  Args:
    X (2D array): Data matrix
    y (1D array): Label vector
    model (sklearn estimator): Classifier with trained weights.

  Returns:
    accuracy (float): Proportion of correct predictions.  
  """
  #############################################################################
  # TODO Complete the function, then remove the next line to test it
  # raise NotImplementedError("Implement the compute_accuracy function")
  #############################################################################

  y_pred = model.predict(X)
  accuracy = np.mean(y_pred == y)

  return accuracy

# Uncomment and run to test your function:
train_accuracy = compute_accuracy(X, y_response, log_reg)
print(f"Accuracy on the training data: {train_accuracy:.2%}")


# instead of using all the cells in VISam, use all the cells from the subnetworks we've identified





lists = sorted(all_trials_window_spikes[300].items()) #flatten dictionary for plotting
x, y = zip(*lists)
plt.eventplot(y, 'horizontal', color='k')
plt.axvline(x=0.5, color='r', linestyle='--')
plt.axvline(x=dat['response_time'][300], color ='b', linestyle='--')
plt.xlabel('Time (s)')
plt.ylabel('Cell number')


adjacency_matrix = all_trials_window_counts.T.corr()

sns.heatmap(adjacency_matrix,
  fmt='.1g',
  vmin=-1, vmax=1, center=0,
  cmap='coolwarm',
  yticklabels=True, xticklabels=True).set_title(region)
plt.show()

test_train = all_trials_window_spikes[0][637]

one_trial_isi = []
for cell in neuron_ids:
  if len(all_trials_window_spikes[0][cell] >= 2:
    isi = np.diff(all_trials_window_spikes[0][cell]
  else:
    isi = np.nan
)


auto_corr = np.correlate(test_train, test_train, mode="full")
auto_corr = auto_corr[auto_corr.size//2:]


isi = np.diff(test_train)



'''
jane test
'''

from curated_data_loader import *
from subnetwork_finder import *

if __name__ == "__main__":

    region = "VISam"
    extracted_data = CuratedDataLoader().spikes_in_decision_time_per_neuron(region)
    """data = CuratedDataLoader.spike_trains_decision_time_per_neuron(region)

    finder = SubnetworkFinder()

    adjacency_matrices_all_bins = list()
    for activity_for_bin in data.activity_matrices:
        adjacency_matrices_all_bins.append(finder.find_network_by_linear_correlation_full_window(activity_for_bin))

    averaged_across_bins = sum(adjacency_matrices_all_bins)/len(data.activity_matrices)"""

    finder = SubnetworkFinder()
    adjacency_matrix = finder.find_network_by_linear_correlation_full_window(extracted_data.activity_matrix)
    # TODO: other measures of the graph - clustering coefficient? characteristic path length?

    visualiser = SubnetworkVisualiser(region)
    visualiser.create_heatmap_from_adjacency_matrix(adjacency_matrix)

    cells = finder.find_functional_subnetwork(adjacency_matrix)
    cells_in_network = CuratedDataLoader.neuron_id_to_cell_type(cells,
                                                                "data/CellMeasures_" +
                                                                extracted_data.session.mouse_name + "_" +
                                                                extracted_data.session.session_date +
                                                                ".csv")

    visualiser.create_histogram_of_cell_types(cells_in_network)

# %%
